{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmXN5bj_ZGWP"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "WvIbn8mvZGWS"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "kfXhAGz6ZGWU"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSuEZqHvZGWU"
      },
      "source": [
        "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "5A_ukMV1ZGWU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "b493248d-173c-4918-e873-450f3e4274c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa645c71390>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBhc9QGZGWV"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "195iGF1hZGWV"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7kU7meEoZGWW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "9DgXS3t5ZGWW"
      },
      "outputs": [],
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "  # <YOUR CODE: define a neural network that predicts policy logits>\n",
        "   nn.Linear(in_features=4, out_features=256, bias=True),\n",
        "   nn.ReLU(),\n",
        "   nn.Linear(in_features=256, out_features=128, bias=True),\n",
        "   nn.ReLU(),\n",
        "   nn.Linear(in_features=128, out_features=n_actions, bias=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if cuda == 'cpu':\n",
        "    print('cpu')\n",
        "else:\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq0rMZZ2gbZT",
        "outputId": "6822b6eb-0b92-44d6-e43b-a776a032a463"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbwBHD6CgdsR",
        "outputId": "b60f35bd-acac-49ea-e522-9511a47367fc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=128, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYABksYsZGWW"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufk5IYeEZGWX"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "od44RRD_ZGWX"
      },
      "outputs": [],
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    \"<YOUR CODE>\"\n",
        "    n = states.shape[0]\n",
        "    states = torch.tensor(states, dtype=torch.float32, device=cuda)\n",
        "    qvalues = model(states) \n",
        "    qvalues_proba = nn.functional.softmax(qvalues, dim=1)\n",
        "\n",
        "    \n",
        "    return qvalues_proba.cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.reset())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mnza8pDQmYK7",
        "outputId": "5aa1f8c6-5bd3-4907-8ca1-44c0e658f0b6"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.02397329  0.03342496  0.03689595  0.04836002]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "hF9KYlxIZGWY"
      },
      "outputs": [],
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(\n",
        "    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (\n",
        "    test_states.shape[0], env.action_space.n), \"wrong output shape: {}\".format(np.shape(test_probas))\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxWehIV-ZGWY"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "LdP7ZfclZGWZ"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    play a full session with REINFORCE agent and train at the session end.\n",
        "    returns sequences of states, actions andrewards\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        \"<YOUR CODE>\"\n",
        "        a = np.random.choice((0, 1), p=action_probs)\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "yombX2cSZGWZ"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_l1be-1ZGWZ"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "xxBnrcZWZGWZ"
      },
      "outputs": [],
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    take a list of immediate rewards r(s,a) for the whole session \n",
        "    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
        "    and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    # <YOUR CODE>\n",
        "    l = len(rewards)\n",
        "    G = list(range(l))\n",
        "\n",
        "    G[l-1] = rewards[-1]\n",
        "\n",
        "    for t in reversed(range(l-1)):\n",
        "      G[t] = rewards[t] + gamma * G[t+1]\n",
        "   \n",
        "  \n",
        "    return G # <YOUR CODE: array of cumulative rewards>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "L665XNyrZGWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "470621c8-8b6d-4ae4-b76f-23b5d41f1ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "looks good!\n"
          ]
        }
      ],
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
        "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards(\n",
        "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards(\n",
        "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mujDRF3MZGWa"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "2tgZdr9TZGWa"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.cuda.\n",
        "                             LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims, device=cuda).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "waLj46onZGWa"
      },
      "outputs": [],
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    Confer: https://fosterelli.co/entropy-loss-for-reinforcement-learning\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32, device=cuda)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32, device=cuda)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32, device=cuda)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    \"<YOUR CODE>\"\n",
        "    entropy = -torch.sum(log_probs * probs).to(cuda)\n",
        "    #print(entropy)\n",
        "    \"<YOUR CODE>\"\n",
        "    J_hat = torch.mean(log_probs_for_actions * cumulative_returns).to(cuda)\n",
        "    # maximize J value, minimize entropy\n",
        "    loss = - J_hat + entropy_coef * entropy   # this implementation does not use baseline\n",
        "\n",
        "\n",
        "    # Gradient descent step\n",
        "    \"<YOUR CODE>\"\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yv4bPmmZGWa"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "nSUC69tzZGWb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb46a2b-2335-49c8-ebb9-8fa19be9b8be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean reward:46.640\n",
            "mean reward:86.650\n",
            "mean reward:52.640\n",
            "mean reward:142.490\n",
            "mean reward:188.770\n",
            "mean reward:375.310\n",
            "You Win!\n"
          ]
        }
      ],
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env))\n",
        "               for _ in range(100)]  # generate new sessions\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    if np.mean(rewards) > 300:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M0eSzsVZGWb"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "hjaBjI1XZGWb"
      },
      "outputs": [],
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "TLJM4bWRZGWb",
        "colab": {
          "resources": {
            "http://localhost:8080/videos/openaigym.video.0.76.video000064.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "a005422d-b4fc-495a-dfef-a83157ee4e91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"480\" controls>\n",
              "  <source src=\"videos/openaigym.video.0.76.video000064.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Tz1lLX-4ZGWb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01333856-211f-4bb0-f3ce-62e2dc684752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your average reward is 317.9 over 100 episodes\n",
            "Submitted to Coursera platform. See results on assignment page!\n"
          ]
        }
      ],
      "source": [
        "from submit import submit_cartpole\n",
        "submit_cartpole(generate_session, 'vlad.kuzevanov@mail.ru', 'aywdIFBlmbdYJSKd')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wtyT_jVZGWb"
      },
      "source": [
        "That's all, thank you for your attention!\n",
        "\n",
        "Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "Copy of practice_reinforce_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}